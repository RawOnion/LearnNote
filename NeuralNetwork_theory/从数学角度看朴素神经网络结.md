![image-20200423185455395](C:\Users\BirdMan\AppData\Roaming\Typora\typora-user-images\image-20200423185455395.png)

神经网络的层数是指隐藏层的数目。



![](https://cdn.jsdelivr.net/gh/RawOnion/imgcloud/img/神经网络结构1.PNG)

由于历史的原因，神经网络也叫多层感知机(multilayer perceptron MLP)。(其实感知机的输入输出一般是二元的0或1，与神经网络中的神经元还是有区别)。

理想化的神经网络所提取的特征是：

```mermaid
graph LR
a(低层网络提取局部细节特征 低级特征)--组合细节特征-->b(高层网络提取全局判别特征 高级特征)
```

也就是说理想化训练出来的权重和偏置所提取的特征将其可视化后是一些局部细节，如短边、点等，但是实际上权重与偏置的意义并不是如此，将网络训练出来的权重提取图像特征后，将其可视化，是一些没有规律的图形，这就导致网络存在不确定性，如果给网络喂进一些噪声图像，网络有时仍会给出一个准确判别。虽然提取的特征将其可视化后与理想的特征可视化有差距，但是这些提取的特征对于有意义图像的判别能很好的完成任务。(为了使网络提取的特征达到理想化特征的标准，因此发展出来了卷积神经网络)

第二层中神经元激活值的计算矩阵表示为：

$$
a^{(1)}=\sigma \left( \left[ \begin{matrix} w_{0,0} & w_{0,1} & \cdots & w_{0,n}\\ w_{1,0}& w_{1,1} & \cdots& w_{1,n}  \\ \vdots & \vdots &\ddots & \vdots  \\ w_{k,0} & w_{k,1} & \cdots & w_{k,n} \end{matrix} \right]  \left[ \begin{matrix} a^{(0)}_0 \\  a^{(0)}_1 \\ \vdots \\ a^{(0)}_n   \end{matrix}  \right] +\left[ \begin{matrix} b_0 \\  b_1 \\ \vdots \\ b_n   \end{matrix}  \right]  \right) 
$$


$a^{(1)}$表示第二层神经元，$a^{(0)}_0$表示第一层第一个神经元，$\sigma()$表示激活函数，会对表达式结果向量中的每一项都计算一次激活函数，$b_0$表示偏置。

$$
a^{(1)}=\sigma(wa^{(0)})+b
$$


整个神经网络其实就可以看做一个函数，而权重和偏置为这个函数的参数：

$$
f(a_0,\cdots,a_{783})=\left[ \begin{matrix} y_0 \\ \vdots \\ y_9 \end{matrix}  \right] 
$$

----

在训练网络时，第一个`epoch`把所有训练样本送入网络中，计算每个样本的损失，并求所有样本损失的平均值，这个平均值就是"经验风险"，用来评价这个网络的好坏。然后依据这个平均损失来反向传播，计算损失函数梯度。

$$
C(w,b)=\frac{1}{2n}\sum\limits_{x}||y(x)-a||^2 
$$


$C$为损失函数(均方差MSE即mean squared error)，$w$表示所有权重，$b$表示所有偏置，$n$训练输入的样本数，$a$是样本$x$输入送进网络时的输出向量，求和是对所有的训练样本进行求和。



为什么要用损失函数与网络权重和偏置构建函数关系，而不是直接使用正确分类的数量与网络权重和偏置构建函数关系？正确分类的图像数量没办法与网络中权重和偏差构建成平滑函数(smooth function)，在大多数情况下，对权重和偏差进行微小的改变，根本不会导致正确分类的训练图像数量发生任何变化。而损失函数可以与网络权重和偏置构建成一个平滑函数，那么就很容易做到使权重和偏差进行小的改变，而改善损失值。

**平滑函数(smooth function)**：所有的权重和偏置变化了很小的权重$\Delta w_j$和偏置$\Delta b_j$就会导致神经元的输出有一个很小的变化$\Delta output$。例如激活函数`sigmoid`便是一个平滑函数。二元微分是当自变量增量无穷小时，以平面来代替曲面。(偏导数本质上是一元导数）
$$
\Delta output\approx \sum\limits_j \frac{\partial output}{\partial w_j}\Delta w_j+\frac{\partial output}{\partial b}\Delta b
$$
![](https://cdn.jsdelivr.net/gh/RawOnion/imgcloud/img/sigmoid函数.PNG)



神经网络函数：

```txt
input(x):pixels像素
output(y):预测值
parameters: 权重和偏置
```



损失函数：

```txt
input(x):权重和偏置
output(y):损失值
parameters: 训练样本
```



**如果输入神经元激活值很低，或者输出神经元已经饱和（过高或者过低的激活值），权重学习缓慢。**

# 参考

[1] [Using neural nets to recognize handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html)

[2] [偏导数](https://www.bilibili.com/s/video/BV1FE411P7MQ)

[3] [动画带你理解偏导数和梯度](https://www.bilibili.com/video/BV1sW411775X?spm_id_from=333.905.b_72656c61746564.5)

[4] [全微分几何意义](https://wenku.baidu.com/view/67122e25f111f18583d05ab8.html)

[5] [求解全微分的意义？最好感性一点的认识](https://www.zhihu.com/question/31464934)