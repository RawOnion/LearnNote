# 随机梯度下降与下批量随机梯度下降

在梯度下降法中，**批量**指得是用于在单次迭代中计算梯度（在损失导函数中针对变量计算一次梯度）的样本总数。
假定批量是指整个数据集，数据集通常包含很大样本（数万甚至数千亿），此外，数据集通常包含多个特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。

**随机梯度下降法（SGD）** 每次迭代只使用一个样本（批量大小为1），如果进行足够的迭代，SGD也可以发挥作用。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法（小批量SGD）** 是介于全批量迭代与SGD之间的折中方案。小批量通常包含10-1000个随机选择的样本。小批量SGD可以减少SGD中的杂乱样本数量，但是仍比全批量更高效。



# 学习率

- 学习率的作用：控制参数更新幅度

- 如果学习率设置过大，可能导致参数在极值附近来回摇摆，无法保证收敛
- 如果学习率设置过小，虽然能保证收敛，但优化速度会大大降低，我们需要更多迭代次数才能达到较理想的优化效果。 



# 梯度下降优化算法

未经过优化的梯度下降算法存在着四个问题<sup>[2]</sup>：
+ 怎么选择学习率。
+ 制定学习率下降计划表或者相邻两代目标函数的值变化低于阈值后改变学习率，但是这都是提前根据经验制定的并不能适应数据的特征。
+ 对所有参数的更新采用形同的学习率，如果有的参数变化大，而有的参数变化小，这样采用相同的学习率来更新所有的参数并不合理。
+ 难以解决对于存在鞍点的非凸函数

**SGD**梯度下降在到达(局部)最优点的附近会出现强烈的震荡，因此**Momentum**方法在**SGD**基础上添加了动量，动量是指对前面速度的积累，参数在更新时候会减去这个动量，相比之前的**SGD**，梯度下降的速度会更快。

**Nesterov accelerated gradient**优化算法针对加了动量的**SGD**算法在遇到坡起的这种情况时不会提前减速的这种情况做出了改进。

现在有了前面两种算法可以根据损失函数的斜率来调整参数的更新，下一步是根据每一个参数的重要性来调整更新，以针对参数进行较大或较小的更新。



**Adagrad**是基于梯度的优化算法，使学习率适应参数。对频繁出现的特征参数用较小学习率，对不频繁出现的特征参数执行用较大的学习率，其优势是不需要手动调整学习率，但是其在分母中积累了平方梯度，这会导致学习率缩小并最终变得无限小，此时算法无法再获取额外的知识。

**Adadelta**是Adagrad的延伸，弥补了其缺点。

**RMSprop**也是为了解决Adagrad的问题，该方法与Adadelta同时研究，但并未发表。

**Adam**是另一种为每个参数自适应学习率的算法，可以认为是结合了RMSprop与Momentum

**Nadam**则结合了NAG和Adam算法的思想。


优化器的选择思路：
**Adam**>**RMSprop**=**Adadelta**>**Adagrad**>**NAG**>**Momentum**>**SGD**

**这些优化梯度下降的算法都是在SGD的基础上进行的优化，优化的思路不尽相同。**



# 参考

[1] [批量梯度下降、随机梯度下降、小批量梯度下降的理解](https://www.cnblogs.com/lliuye/p/9451903.html)

[2] [梯度下降优化算法综述](https://blog.csdn.net/google19890102/article/details/69942970)

